{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red25\green25\blue25;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c12941\c12941\c12941;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\
  _warn_prf(average, modifier, msg_start, len(result))\
(\{'acc': 0.516010498687664,\
  'confusion_abs': [[241, 0, 15, 5, 25, 116],\
   [60, 0, 11, 5, 19, 62],\
   [8, 0, 61, 0, 2, 23],\
   [15, 0, 4, 142, 43, 52],\
   [39, 0, 18, 8, 109, 144],\
   [59, 0, 21, 37, 62, 402]],\
  'labels': [0, 1, 2, 3, 4, 5],\
  'macro-f1': 0.4463109742213973,\
  'macro-precision': 0.46019709531786646,\
  'macro-recall': 0.45465712691457266,\
  'micro-f1': 0.516010498687664,\
  'micro-precision': 0.516010498687664,\
  'micro-recall': 0.516010498687664,\
  'per-label-f1': [0.5758661887694145,\
   0.0,\
   0.5398230088495576,\
   0.6173913043478261,\
   0.37328767123287676,\
   0.5698086463501063],\
  'per-label-precision': [0.5552995391705069,\
   0.0,\
   0.4621212121212121,\
   0.696078431372549,\
   0.41132075471698115,\
   0.4878640776699029],\
  'per-label-recall': [0.598014888337469,\
   0.0,\
   0.648936170212766,\
   0.5546875,\
   0.34169278996865204,\
   0.6848381601362862],\
  'weighted-f1': 0.4880934663253418,\
  'weighted-precision': 0.47826544751977423,\
  'weighted-recall': 0.516010498687664\},\
 [[59.95, 0.0, 3.73, 1.24, 6.22, 28.86],\
  [38.22, 0.0, 7.01, 3.18, 12.1, 39.49],\
  [8.51, 0.0, 64.89, 0.0, 2.13, 24.47],\
  [5.86, 0.0, 1.56, 55.47, 16.8, 20.31],\
  [12.26, 0.0, 5.66, 2.52, 34.28, 45.28],\
  [10.15, 0.0, 3.61, 6.37, 10.67, 69.19]],\
 '              precision    recall  f1-score   support\\n\\n           0     0.5553    0.5980    0.5759       403\\n           1     0.0000    0.0000    0.0000       167\\n           2     0.4621    0.6489    0.5398        94\\n           3     0.6961    0.5547    0.6174       256\\n           4     0.4113    0.3417    0.3733       319\\n           5     0.4879    0.6848    0.5698       587\\n           6     0.6087    0.3544    0.4480        79\\n\\n    accuracy                         0.5160      1905\\n   macro avg     0.4602    0.4547    0.4463      1905\\nweighted avg     0.4783    0.5160    0.4881      1905\\n')\
\
\
\
======== Epoch 1 / 5 ========\
Training...\
  Batch    40  of    587.    Elapsed: 0:00:25.\
  Batch    80  of    587.    Elapsed: 0:00:50.\
  Batch   120  of    587.    Elapsed: 0:01:15.\
  Batch   160  of    587.    Elapsed: 0:01:39.\
  Batch   200  of    587.    Elapsed: 0:02:04.\
  Batch   240  of    587.    Elapsed: 0:02:29.\
  Batch   280  of    587.    Elapsed: 0:02:54.\
  Batch   320  of    587.    Elapsed: 0:03:18.\
  Batch   360  of    587.    Elapsed: 0:03:43.\
  Batch   400  of    587.    Elapsed: 0:04:08.\
  Batch   440  of    587.    Elapsed: 0:04:33.\
  Batch   480  of    587.    Elapsed: 0:04:58.\
  Batch   520  of    587.    Elapsed: 0:05:22.\
  Batch   560  of    587.    Elapsed: 0:05:47.\
\
  Average training loss: 24.94\
  Training epcoh took: 0:06:04\
\
======== Epoch 2 / 5 ========\
Training...\
  Batch    40  of    587.    Elapsed: 0:00:25.\
  Batch    80  of    587.    Elapsed: 0:00:50.\
  Batch   120  of    587.    Elapsed: 0:01:15.\
  Batch   160  of    587.    Elapsed: 0:01:40.\
  Batch   200  of    587.    Elapsed: 0:02:04.\
  Batch   240  of    587.    Elapsed: 0:02:29.\
  Batch   280  of    587.    Elapsed: 0:02:54.\
  Batch   320  of    587.    Elapsed: 0:03:19.\
  Batch   360  of    587.    Elapsed: 0:03:44.\
  Batch   400  of    587.    Elapsed: 0:04:09.\
  Batch   440  of    587.    Elapsed: 0:04:34.\
  Batch   480  of    587.    Elapsed: 0:04:59.\
  Batch   520  of    587.    Elapsed: 0:05:23.\
  Batch   560  of    587.    Elapsed: 0:05:48.\
\
  Average training loss: 21.24\
  Training epcoh took: 0:06:05\
\
======== Epoch 3 / 5 ========\
Training...\
  Batch    40  of    587.    Elapsed: 0:00:25.\
  Batch    80  of    587.    Elapsed: 0:00:50.\
  Batch   120  of    587.    Elapsed: 0:01:14.\
  Batch   160  of    587.    Elapsed: 0:01:39.\
  Batch   200  of    587.    Elapsed: 0:02:04.\
  Batch   240  of    587.    Elapsed: 0:02:29.\
  Batch   280  of    587.    Elapsed: 0:02:54.\
  Batch   320  of    587.    Elapsed: 0:03:19.\
  Batch   360  of    587.    Elapsed: 0:03:44.\
  Batch   400  of    587.    Elapsed: 0:04:09.\
  Batch   440  of    587.    Elapsed: 0:04:33.\
  Batch   480  of    587.    Elapsed: 0:04:58.\
  Batch   520  of    587.    Elapsed: 0:05:23.\
  Batch   560  of    587.    Elapsed: 0:05:48.\
\
  Average training loss: 18.10\
  Training epcoh took: 0:06:05\
\
======== Epoch 4 / 5 ========\
Training...\
  Batch    40  of    587.    Elapsed: 0:00:25.\
  Batch    80  of    587.    Elapsed: 0:00:50.\
  Batch   120  of    587.    Elapsed: 0:01:15.\
  Batch   160  of    587.    Elapsed: 0:01:39.\
  Batch   200  of    587.    Elapsed: 0:02:04.\
  Batch   240  of    587.    Elapsed: 0:02:29.\
  Batch   280  of    587.    Elapsed: 0:02:54.\
  Batch   320  of    587.    Elapsed: 0:03:19.\
  Batch   360  of    587.    Elapsed: 0:03:43.\
  Batch   400  of    587.    Elapsed: 0:04:08.\
  Batch   440  of    587.    Elapsed: 0:04:33.\
  Batch   480  of    587.    Elapsed: 0:04:58.\
  Batch   520  of    587.    Elapsed: 0:05:23.\
  Batch   560  of    587.    Elapsed: 0:05:47.\
\
  Average training loss: 15.63\
  Training epcoh took: 0:06:04\
\
======== Epoch 5 / 5 ========\
Training...\
  Batch    40  of    587.    Elapsed: 0:00:25.\
  Batch    80  of    587.    Elapsed: 0:00:50.\
  Batch   120  of    587.    Elapsed: 0:01:14.\
  Batch   160  of    587.    Elapsed: 0:01:39.\
  Batch   200  of    587.    Elapsed: 0:02:04.\
  Batch   240  of    587.    Elapsed: 0:02:29.\
  Batch   280  of    587.    Elapsed: 0:02:54.\
  Batch   320  of    587.    Elapsed: 0:03:19.\
  Batch   360  of    587.    Elapsed: 0:03:43.\
  Batch   400  of    587.    Elapsed: 0:04:08.\
  Batch   440  of    587.    Elapsed: 0:04:33.\
  Batch   480  of    587.    Elapsed: 0:04:58.\
  Batch   520  of    587.    Elapsed: 0:05:23.\
  Batch   560  of    587.    Elapsed: 0:05:48.\
\
  Average training loss: 13.74\
  Training epcoh took: 0:06:04\
\
Training complete!\
Total training took 0:30:22 (h:mm:ss)}